# Manual-RAG Diagnostic Assistant — Configuration
# =================================================

# Ollama server URL (default: local)
OLLAMA_BASE_URL=http://localhost:11434

# Default LLM model — see System Guide for full list
# Recommended: llama3.3:8b (16GB RAM) or qwen2.5:7b
LLM_MODEL=llama3.3:8b

# Embedding model — BGE-small recommended for best retrieval accuracy
# Options: BAAI/bge-small-en-v1.5, all-MiniLM-L6-v2, BAAI/bge-base-en-v1.5
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# ChromaDB storage path
CHROMA_PERSIST_DIR=./chroma_db

# Chunk size for document splitting (characters)
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# ---------------------------------------------------------------------------
# Retrieval Quality Filter
# ---------------------------------------------------------------------------
# Minimum relevance score (0-100) to include a chunk in LLM context.
# Chunks below this threshold are discarded as noise.
# Uses cosine similarity derived from L2 distance on normalized embeddings.
#   40 = recommended (catches useful chunks, filters junk)
#   50 = strict (fewer but higher quality)
#   0  = disabled (send everything to LLM)
MIN_RELEVANCE_SCORE=40

# Vision model for diagram/image description (used during PDF processing)
# Options: minicpm-v (best for documents), llava, llava-phi3, llama3.2-vision
# If unavailable, falls back to Tesseract OCR automatically
VISION_MODEL=minicpm-v

# Directory for storing extracted diagram images
IMAGE_STORE_DIR=./images

# ---------------------------------------------------------------------------
# LLM Timeout & Fallback Configuration
# ---------------------------------------------------------------------------
# Max seconds to wait for the FIRST token from LLM (model loading can be slow)
LLM_FIRST_TOKEN_TIMEOUT=120

# Max seconds allowed between consecutive tokens during streaming
LLM_INTER_TOKEN_TIMEOUT=60

# Total wall-clock limit (seconds) for a single response. 0 = unlimited.
LLM_MAX_RESPONSE_TIME=600

# Fallback model — auto-switches if primary model times out.
# Use a smaller/faster model. Leave empty to disable fallback.
# Example: if primary is qwen2.5:7b, fallback could be phi3:3.8b
FALLBACK_MODEL=
