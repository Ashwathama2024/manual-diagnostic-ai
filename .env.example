# Manual-RAG Diagnostic Assistant — Configuration
# =================================================

# Ollama server URL (default: local)
OLLAMA_BASE_URL=http://localhost:11434

# Default LLM model — see System Guide for full list
# Recommended: llama3.3:8b (16GB RAM) or qwen2.5:7b
LLM_MODEL=llama3.3:8b

# Embedding model — BGE-small recommended for best retrieval accuracy
# Options: BAAI/bge-small-en-v1.5, all-MiniLM-L6-v2, BAAI/bge-base-en-v1.5
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# ChromaDB storage path
CHROMA_PERSIST_DIR=./chroma_db

# Chunk size for document splitting (characters)
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Vision model for diagram/image description (used during PDF processing)
# Options: minicpm-v (best for documents), llava, llava-phi3, llama3.2-vision
# If unavailable, falls back to Tesseract OCR automatically
VISION_MODEL=minicpm-v

# Directory for storing extracted diagram images
IMAGE_STORE_DIR=./images
